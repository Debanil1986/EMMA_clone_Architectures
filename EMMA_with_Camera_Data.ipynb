{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debanil1986/Waymo_indivitualProject/blob/main/EMMA_with_Camera_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A58OjlgfwMer"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Reduce spatial size by half\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Further reduce spatial size\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling to output 1x1 feature map\n",
        "        )\n",
        "        self.fc = nn.Linear(256, output_dim)  # Fully connected layer to reduce to output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten the spatial dimensions\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ji5w_T1ZJtuF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "\n",
        "def load_pretrained_object_detector():\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "object_detector = load_pretrained_object_detector()  # Load the model at the start of your script or main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCeKbK2rmG2m",
        "outputId": "621f78f7-523a-4daf-d358-35009d25f0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:3000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "[2025-02-26 21:18:51,304] ERROR in app: Exception on /convert-video-to-base64 [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/flask/app.py\", line 1511, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/flask/app.py\", line 919, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/flask/app.py\", line 917, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/debanilguha/Library/Mobile Documents/com~apple~CloudDocs/GERMAN/INGOLSTADT HOCHSCHULE/INGOLSTADT_SEMESTER_2/Teamprojekte_WINTERSEMESTER_2024/NEURAL_NETWORK_IDEAS/Waymo_indivitualProject/.venv/lib/python3.11/site-packages/flask/app.py\", line 902, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/jz/jdxddpys7g1_q5rl1d4dbpcw0000gn/T/ipykernel_8176/250863239.py\", line 200, in convert_video_to_base64\n",
            "    emma_model = EMMA(cnn_feature_dim, intent_dim, historical_state_dim, hidden_size)  # Initialize the EMMA model\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/jz/jdxddpys7g1_q5rl1d4dbpcw0000gn/T/ipykernel_8176/250863239.py\", line 18, in __init__\n",
            "    self.cnn = CustomCNN(output_dim=self.cnn_feature_dim)  # Use the custom CNN\n",
            "               ^^^^^^^^^\n",
            "NameError: name 'CustomCNN' is not defined\n",
            "127.0.0.1 - - [26/Feb/2025 21:18:51] \"\u001b[35m\u001b[1mPOST /convert-video-to-base64 HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from flask import Flask, request, redirect, url_for,jsonify\n",
        "from flask_cors import CORS,cross_origin\n",
        "from werkzeug.utils import secure_filename\n",
        "import asyncio\n",
        "from asgiref.wsgi import WsgiToAsgi\n",
        "import time\n",
        "\n",
        "\n",
        "cnn_feature_dim = 512\n",
        "intent_dim = 10\n",
        "historical_state_dim = 4\n",
        "hidden_size = 512\n",
        "resized_width, resized_height = 640, 480\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "output_video_bytes = None\n",
        "UPLOAD_FOLDER = './'\n",
        "ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov'}\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "app.config['MAX_CONTENT_LENGTH'] = 20 * 1024 * 1024\n",
        "\n",
        "progress_updates = []\n",
        "\n",
        "\n",
        "class EMMA:\n",
        "    def __init__(self, cnn_feature_dim, intent_dim, historical_state_dim,hidden_size):\n",
        "        super(EMMA, self).__init__()\n",
        "        self.cnn_feature_dim = 512  # Desired output feature size from CNN\n",
        "        self.cnn = CustomCNN(output_dim=self.cnn_feature_dim)  # Use the custom CNN\n",
        "        self.rnn_input_size = cnn_feature_dim + intent_dim + historical_state_dim\n",
        "        self.rnn = torch.nn.LSTM(input_size=self.rnn_input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, 2)\n",
        "        \n",
        "    def state_dict(self):\n",
        "        state_dict = {\n",
        "        'cnn.conv_layers.0.weight': self.cnn.conv_layers[0].weight,\n",
        "        'cnn.conv_layers.0.bias': self.cnn.conv_layers[0].bias,\n",
        "        'cnn.conv_layers.3.weight': self.cnn.conv_layers[3].weight,\n",
        "        'cnn.conv_layers.3.bias': self.cnn.conv_layers[3].bias,\n",
        "        'cnn.conv_layers.6.weight': self.cnn.conv_layers[6].weight,\n",
        "        'cnn.conv_layers.6.bias': self.cnn.conv_layers[6].bias,\n",
        "        'fc.weight': self.fc.weight,\n",
        "        'fc.bias': self.fc.bias\n",
        "        }\n",
        "        return state_dict\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Resize and normalize the frame.\"\"\"\n",
        "        # Example preprocessing: resize and normalize\n",
        "        resized_frame = cv2.resize(frame, (resized_width, resized_height))\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "        return normalized_frame\n",
        "\n",
        "    def predict(self, frame, intents, historical_states):\n",
        "        \"\"\"Make a prediction using the preprocessed frame.\"\"\"\n",
        "        # Convert frame to a batch format (batch size 1)\n",
        "        camera_frames = frame\n",
        "\n",
        "        camera_frames_tensor = torch.tensor(camera_frames, dtype=torch.float32)\n",
        "\n",
        "        batch_size, T, W, H, C = camera_frames_tensor.shape\n",
        "        cnn_out = self.cnn(camera_frames_tensor.view(-1, C, H, W))  # Reshape and pass through CNN\n",
        "        cnn_out = cnn_out.view(batch_size, T, -1)\n",
        "\n",
        "        # Combine CNN output with intents and historical_states\n",
        "        # Here you might need to encode intents and concatenate\n",
        "        intents_tensor = torch.tensor(intents, dtype=torch.float32)  # Shape: (batch, time, intent_dim)\n",
        "        historical_states_tensor = torch.tensor(historical_states, dtype=torch.float32)  # Shape: (batch, time, state_dim)\n",
        "\n",
        "        combined_features = torch.cat((cnn_out, intents_tensor, historical_states_tensor), dim=-1)\n",
        "\n",
        "        rnn_out, _ = self.rnn(combined_features)\n",
        "        output = self.fc(rnn_out)\n",
        "        return output\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Extract frames from a video and process them with the model.\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            preprocessed_frame = self.preprocess_frame(frame)\n",
        "            intents = np.random.rand(1, 1, intent_dim)  # Random intents\n",
        "            historical_states = np.random.rand(1, 1, historical_state_dim)\n",
        "            output = self.predict(preprocessed_frame, intents, historical_states)\n",
        "            print(output)  # Print or further process the output\n",
        "            # Display frame\n",
        "            cv2.imshow('Video Frame', frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Resize and normalize the frame.\"\"\"\n",
        "    # Resize the frame to the required input size of the model\n",
        "    resized_frame = cv2.resize(frame, (resized_width, resized_height))  # Example resize\n",
        "    # Normalize the frame if necessary\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=0)  # Add batch dimension\n",
        "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)  # Add time dimension\n",
        "    return preprocessed_frame\n",
        "\n",
        "def draw_lane_overlay(frame, lane_points):\n",
        "    \"\"\"Draws a semi-transparent lane overlay.\"\"\"\n",
        "    overlay = frame.copy()\n",
        "    cv2.fillPoly(overlay, [np.array(lane_points, np.int32)], (0, 255, 0))\n",
        "    alpha = 0.4  # Transparency factor.\n",
        "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
        "\n",
        "def adjust_lane_points(frame_width, frame_height):\n",
        "    # Example adjustment, these points should be dynamically calculated based on actual lane detection\n",
        "    return [\n",
        "        (frame_width * 0.4, frame_height),  # Bottom left\n",
        "        (frame_width * 0.6, frame_height),  # Bottom right\n",
        "        (frame_width * 0.55, frame_height * 0.7),  # Top right\n",
        "        (frame_width * 0.45, frame_height * 0.7)   # Top left\n",
        "    ]\n",
        "\n",
        "def preprocess_frame_for_torch(frame):\n",
        "    \"\"\"Preprocess the frame for PyTorch model input.\"\"\"\n",
        "    # Convert frame to RGB (PyTorch models expect RGB)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Resize the frame to the required input size of the model\n",
        "    resized_frame = cv2.resize(frame_rgb, (resized_width, resized_height))  # Example resize\n",
        "    # Normalize the frame to 0-1\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    # Convert to tensor\n",
        "    tensor_frame = torch.from_numpy(normalized_frame).float()\n",
        "    # Rearrange dimensions to (C, H, W) from (H, W, C)\n",
        "    tensor_frame = tensor_frame.permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
        "    return tensor_frame\n",
        "\n",
        "\n",
        "def process_video(video_path, output_video_path, model, intent_dim, historical_state_dim):\n",
        "    global progress_updates\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    lengthOfFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to open the video.\")\n",
        "        return\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
        "    pbar = tqdm(total=lengthOfFrames, unit=\"frames\")\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        preprocessed_frame = preprocess_frame(frame)\n",
        "        preprocessed_tensor = preprocess_frame_for_torch(frame)\n",
        "        with torch.no_grad():\n",
        "          detection_output = object_detector(preprocessed_tensor)\n",
        "          detections = detection_output[0]\n",
        "\n",
        "        # Draw detections with high confidence scores\n",
        "\n",
        "        labels = detections['labels'].cpu().numpy()\n",
        "        boxes = detections['boxes'].cpu().numpy()\n",
        "        scores = detections['scores'].cpu().numpy()\n",
        "        scale_width = frame_width / resized_width\n",
        "        scale_height = frame_width / resized_height\n",
        "        for label, box, score in zip(labels, boxes, scores):\n",
        "            if score > 0.9:  # Threshold can be adjusted\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                x1 = int(x1 * scale_width)\n",
        "                y1 = int(y1 * scale_height)\n",
        "                x2 = int(x2 * scale_width)\n",
        "                y2 = int(y2 * scale_height)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"Car: {score:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        intents = np.random.rand(1, 1, intent_dim)\n",
        "        historical_states = np.random.rand(1, 1, historical_state_dim)\n",
        "        output = model.predict(preprocessed_frame, intents, historical_states)\n",
        "\n",
        "        lane_points = adjust_lane_points(frame_width, frame_height)\n",
        "        draw_lane_overlay(frame, lane_points)\n",
        "\n",
        "        out.write(frame)\n",
        "        pbar.update(1)\n",
        "        progress_updates.append(f\"Processed frame {len(progress_updates)}/{lengthOfFrames}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Output video saved to {output_video_path}\")\n",
        "    pbar.close()\n",
        "    out.release()\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and \\\n",
        "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "@app.route('/convert-video-to-base64',methods=['POST'])\n",
        "async def convert_video_to_base64():\n",
        "    global progress_updates\n",
        "    progress_updates = []\n",
        "    emma_model = EMMA(cnn_feature_dim, intent_dim, historical_state_dim, hidden_size)  # Initialize the EMMA model\n",
        "    print(request.files)\n",
        "    if 'video' not in request.files:\n",
        "            return 'No file part', 400\n",
        "    file = request.files['video']\n",
        "    if file and allowed_file(file.filename):\n",
        "        filename = secure_filename(\"input.mp4\")\n",
        "        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "    video_path ='input.mp4'\n",
        "    output_video_path = 'emma_processed_videos.mp4'\n",
        "    \n",
        "    process_video(video_path, output_video_path, emma_model, intent_dim, historical_state_dim)\n",
        "    # t = threading.Thread(target=process_video, args=(video_path, output_video_path, emma_model, intent_dim, historical_state_dim))\n",
        "    # t.start()\n",
        "    return jsonify({\"message\": \"Video processing started\"}), 202\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # asgi_app = WsgiToAsgi(app)\n",
        "    app.run(port=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npr5Ui8b_8k2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMOFx7Bzcq4d78NXwC/sxYw",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
